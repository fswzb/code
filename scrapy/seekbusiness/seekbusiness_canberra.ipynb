{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scrapy import signals\n",
    "from scrapy.utils.project import get_project_settings\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.xlib.pydispatch import dispatcher\n",
    "from multiprocessing.queues import Queue\n",
    "import scrapy\n",
    "import multiprocessing\n",
    "import datetime\n",
    "import collections\n",
    "import urllib\n",
    "import collections\n",
    "from scrapy import Request as Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "business_data=collections.namedtuple('business_data','link,title,status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CrawlerWorker(multiprocessing.Process):\n",
    "\n",
    "    def __init__(self, spider, result_queue):\n",
    "        multiprocessing.Process.__init__(self)\n",
    "        self.result_queue = result_queue\n",
    "\n",
    "        self.crawler = CrawlerProcess(get_project_settings())\n",
    "        #if not hasattr(project, 'crawler'):\n",
    "        #    self.crawler.install()\n",
    "        #self.crawler.configure()\n",
    "\n",
    "        self.items = []\n",
    "        self.spider = spider\n",
    "        dispatcher.connect(self._item_passed, signals.item_passed)\n",
    "\n",
    "    def _item_passed(self, item):\n",
    "        self.items.append(item)\n",
    "\n",
    "    def run(self):\n",
    "        self.crawler.crawl(self.spider)\n",
    "        self.crawler.start()\n",
    "        self.crawler.stop()\n",
    "        self.result_queue.put(self.items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CanberraBusinessSpider(scrapy.Spider):\n",
    "    name = \"CanberraBusinessSpider\"\n",
    "    allowed_domains = [\"www.seekbusiness.com.au\"]\n",
    "    start_urls = ['https://www.seekbusiness.com.au/businesses-for-sale/in-canberra-act-2601?rad=50']\n",
    "\n",
    "    def parse(self, response):\n",
    "        next_page=response.xpath('//a[contains(@class,\"next\")]/@href').extract()[0]\n",
    "        next_page_link=urllib.basejoin(response.url,next_page)\n",
    "        #print(next_page_link)\n",
    "        #yield Request(next_page_link,callback=self.parse)\n",
    "        \n",
    "        business_tags=response.xpath('//div[contains(@class,\"sr-l\") and @onclick]')\n",
    "        for business in business_tags:\n",
    "            title=business.xpath('div[@class=\"t\"]/a/text()').extract()[0]\n",
    "            link=business.xpath('div[@class=\"t\"]/a/@href').extract()[0]\n",
    "            full_link=urllib.basejoin(response.url,link)\n",
    "            contents=business.xpath('div[@class=\"m-c\"]/div[@class=\"det-c\"]/div[@class=\"smry\"]/text()').extract()\n",
    "            full_content='\\n'.join(contents)\n",
    "            print title,full_link,full_content\n",
    "        \"\"\"\n",
    "        business_tags=response.xpath('//a[contains(@href,\"business-listing\") and count(@*)=1]')\n",
    "        for invidual_business in business_tags:\n",
    "            title=invidual_business.xpath('text()').extract()[0]\n",
    "            link=invidual_business.xpath('@href').extract()[0]\n",
    "            full_link=urllib.basejoin(response.url,link)\n",
    "            yield {title:business_data(full_link,title,'not yet')}\n",
    "            #business={'title':title,'link':full_link}\n",
    "            #print(business)\n",
    "        #print(business_tags)\n",
    "        \"\"\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-19 09:58:04 [scrapy] INFO: Scrapy 1.1.1 started (bot: scrapybot)\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Scrapy 1.1.1 started (bot: scrapybot)\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Scrapy 1.1.1 started (bot: scrapybot)\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Scrapy 1.1.1 started (bot: scrapybot)\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Scrapy 1.1.1 started (bot: scrapybot)\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Scrapy 1.1.1 started (bot: scrapybot)\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Scrapy 1.1.1 started (bot: scrapybot)\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Scrapy 1.1.1 started (bot: scrapybot)\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Scrapy 1.1.1 started (bot: scrapybot)\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Scrapy 1.1.1 started (bot: scrapybot)\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Overridden settings: {}\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Overridden settings: {}\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Overridden settings: {}\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Overridden settings: {}\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Overridden settings: {}\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Overridden settings: {}\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Overridden settings: {}\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Overridden settings: {}\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Overridden settings: {}\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Overridden settings: {}\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Enabled extensions:\n",
      "['scrapy.extensions.logstats.LogStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.corestats.CoreStats']\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Enabled extensions:\n",
      "['scrapy.extensions.logstats.LogStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.corestats.CoreStats']\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Enabled extensions:\n",
      "['scrapy.extensions.logstats.LogStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.corestats.CoreStats']\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Enabled extensions:\n",
      "['scrapy.extensions.logstats.LogStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.corestats.CoreStats']\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Enabled extensions:\n",
      "['scrapy.extensions.logstats.LogStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.corestats.CoreStats']\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Enabled extensions:\n",
      "['scrapy.extensions.logstats.LogStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.corestats.CoreStats']\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Enabled extensions:\n",
      "['scrapy.extensions.logstats.LogStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.corestats.CoreStats']\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Enabled extensions:\n",
      "['scrapy.extensions.logstats.LogStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.corestats.CoreStats']\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Enabled extensions:\n",
      "['scrapy.extensions.logstats.LogStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.corestats.CoreStats']\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Enabled extensions:\n",
      "['scrapy.extensions.logstats.LogStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.corestats.CoreStats']\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Spider opened\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Spider opened\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Spider opened\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Spider opened\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Spider opened\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Spider opened\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Spider opened\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Spider opened\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Spider opened\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Spider opened\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2017-01-19 09:58:04 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2017-01-19 09:58:04 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6024\n",
      "2017-01-19 09:58:04 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6024\n",
      "2017-01-19 09:58:04 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6024\n",
      "2017-01-19 09:58:04 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6024\n",
      "2017-01-19 09:58:04 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6024\n",
      "2017-01-19 09:58:04 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6024\n",
      "2017-01-19 09:58:04 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6024\n",
      "2017-01-19 09:58:04 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6024\n",
      "2017-01-19 09:58:04 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6024\n",
      "2017-01-19 09:58:04 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6024\n",
      "2017-01-19 09:58:06 [scrapy] DEBUG: Crawled (200) <GET https://www.seekbusiness.com.au/businesses-for-sale/in-canberra-act-2601?rad=50> (referer: None)\n",
      "2017-01-19 09:58:06 [scrapy] DEBUG: Crawled (200) <GET https://www.seekbusiness.com.au/businesses-for-sale/in-canberra-act-2601?rad=50> (referer: None)\n",
      "2017-01-19 09:58:06 [scrapy] DEBUG: Crawled (200) <GET https://www.seekbusiness.com.au/businesses-for-sale/in-canberra-act-2601?rad=50> (referer: None)\n",
      "2017-01-19 09:58:06 [scrapy] DEBUG: Crawled (200) <GET https://www.seekbusiness.com.au/businesses-for-sale/in-canberra-act-2601?rad=50> (referer: None)\n",
      "2017-01-19 09:58:06 [scrapy] DEBUG: Crawled (200) <GET https://www.seekbusiness.com.au/businesses-for-sale/in-canberra-act-2601?rad=50> (referer: None)\n",
      "2017-01-19 09:58:06 [scrapy] DEBUG: Crawled (200) <GET https://www.seekbusiness.com.au/businesses-for-sale/in-canberra-act-2601?rad=50> (referer: None)\n",
      "2017-01-19 09:58:06 [scrapy] DEBUG: Crawled (200) <GET https://www.seekbusiness.com.au/businesses-for-sale/in-canberra-act-2601?rad=50> (referer: None)\n",
      "2017-01-19 09:58:06 [scrapy] DEBUG: Crawled (200) <GET https://www.seekbusiness.com.au/businesses-for-sale/in-canberra-act-2601?rad=50> (referer: None)\n",
      "2017-01-19 09:58:06 [scrapy] DEBUG: Crawled (200) <GET https://www.seekbusiness.com.au/businesses-for-sale/in-canberra-act-2601?rad=50> (referer: None)\n",
      "2017-01-19 09:58:06 [scrapy] DEBUG: Crawled (200) <GET https://www.seekbusiness.com.au/businesses-for-sale/in-canberra-act-2601?rad=50> (referer: None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rent Roll Sale Canberra/Queanbeyan Region (75-100 properties) https://www.seekbusiness.com.au/business-listing/businesses/291164?s=0 Rent Roll Sale Canberra/Queanbeyan Region (75-100 properties)\n",
      "                \n",
      "\n",
      "            \n",
      "CafeCoffeeShop - Canberra https://www.seekbusiness.com.au/business-listing/businesses/265305?s=0 Cafe -    Takeaway  -  Franchise - Canberra\n",
      "                \n",
      "\n",
      "            \n",
      "Mo's Mobiles is looking for Licensed Dealers to operate existing Vodafone stores https://www.seekbusiness.com.au/business-listing/businesses/281849?s=0 Mo’s Mobiles is Vodafone’s largest exclusive dealer. We are looking for Licensees to operate a number of our existing stores.\n",
      "\n",
      "                \n",
      "\n",
      "            \n",
      "Gozleme King Franchise : Turkish Street Food & Cafe https://www.seekbusiness.com.au/business-listing/businesses/270622?s=0 Gözleme King is  offering premium franchise opportunities located in major shopping centers , busy high street's and major CBD's.  Contact us now to find out more.\n",
      "                \n",
      "\n",
      "            \n",
      "Healthy Start Civic https://www.seekbusiness.com.au/business-listing/businesses/258794?s=0 A healthy bargain buy to start your next business.\n",
      "                \n",
      "\n",
      "            \n",
      "The Fat Goanna Cafe https://www.seekbusiness.com.au/business-listing/businesses/290623?s=0 5-day week cafe surrounded by government offices low rent and secure lease.\n",
      "\n",
      "                \n",
      "\n",
      "            \n",
      "Community sector RTO for sale. Delegate Authority. Funding- 2 states. Excellent! https://www.seekbusiness.com.au/business-listing/businesses/286398?s=0 Predominantly servicing the Community services & hospitality sectors - This RTO is showing a first quarter profit of $90k. \n",
      "\n",
      "RTO is reluctantly on the market amidst significant within the business.\n",
      "                \n",
      "\n",
      "            \n",
      "Rent Roll Sale 80-100 Managements https://www.seekbusiness.com.au/business-listing/businesses/264886?s=0 -----SOLD-----\n",
      "                \n",
      "\n",
      "            \n",
      "Lollipop's Playland - Children's Indoor Play centre and Café franchise https://www.seekbusiness.com.au/business-listing/businesses/107888?s=0 Lollipop's Playlands are Australia's largest Indoor play centre business with 20 centres nationally and 31 globally with a proud 20 year history.\n",
      "                \n",
      "\n",
      "            \n",
      "Sumo Salad Tuggeranong: a successful absentee owner food franchise https://www.seekbusiness.com.au/business-listing/businesses/286193?s=0 Secure Food Franchise Opportunity\n",
      "                \n",
      "\n",
      "            \n",
      "Exciting opportunity to own and manage your own Skin & Laser clinic!!! https://www.seekbusiness.com.au/business-listing/businesses/289893?s=0 Clearskincare Clinics Managing Owners can start living the life you've imagined with a 40% profit share and a $85,000 package per year with a proven business model, and ongoing support & training.\n",
      "                \n",
      "\n",
      "            \n",
      "Own your own RTO today! https://www.seekbusiness.com.au/business-listing/businesses/239855?s=0 ----SOLD----\n",
      "                \n",
      "\n",
      "            \n",
      "Southside Supermarket https://www.seekbusiness.com.au/business-listing/businesses/205055?s=0 ---------------SOLD------------\n",
      "                \n",
      "\n",
      "            \n",
      "Ace Sushi Braddon https://www.seekbusiness.com.au/business-listing/businesses/289743?s=0 Under Instructions From The Administrator – ACE Sushi\n",
      "                \n",
      "\n",
      "            \n",
      "Supercheap Storage l Mobile self-storage l Simple management, highly profitable! https://www.seekbusiness.com.au/business-listing/businesses/222538?s=0 Supercheap Storage, mobile self-storage provider with a difference. Simplified, convenient service. Stress-free management. Supercheap Storage - the business in Canberra you have been searching for\n",
      "                \n",
      "\n",
      "            \n",
      "Japanese Tapas Bar https://www.seekbusiness.com.au/business-listing/businesses/289567?s=0 Under Instructions From The Administrator - Japanese Tapas Bar\n",
      "                \n",
      "\n",
      "            \n",
      "Sumo Salad Woden https://www.seekbusiness.com.au/business-listing/businesses/289538?s=0 If you have been searching for a below value business, then stop and start with Sumo Salad Woden.\n",
      "                \n",
      "\n",
      "            \n",
      "CafeCoffeeShop - Canberra https://www.seekbusiness.com.au/business-listing/businesses/269411?s=0 Cafe  -   Takeaway  -   Franchise - Canberra\n",
      "                \n",
      "\n",
      "            \n",
      "Furniture Direct Warehouse https://www.seekbusiness.com.au/business-listing/businesses/289390?s=0 Furniture Direct Warehouse\n",
      "                \n",
      "\n",
      "            \n",
      "Ogilvie Automotive and Exhaust https://www.seekbusiness.com.au/business-listing/businesses/289371?s=0 The seller is prepared to offer $30,000 seller finance to be repaid in small weekly instalments over 2 years.\n",
      "                \n",
      "\n",
      "            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-19 09:58:06 [scrapy] INFO: Closing spider (finished)\n",
      "2017-01-19 09:58:06 [scrapy] INFO: Closing spider (finished)\n",
      "2017-01-19 09:58:06 [scrapy] INFO: Closing spider (finished)\n",
      "2017-01-19 09:58:06 [scrapy] INFO: Closing spider (finished)\n",
      "2017-01-19 09:58:06 [scrapy] INFO: Closing spider (finished)\n",
      "2017-01-19 09:58:06 [scrapy] INFO: Closing spider (finished)\n",
      "2017-01-19 09:58:06 [scrapy] INFO: Closing spider (finished)\n",
      "2017-01-19 09:58:06 [scrapy] INFO: Closing spider (finished)\n",
      "2017-01-19 09:58:06 [scrapy] INFO: Closing spider (finished)\n",
      "2017-01-19 09:58:06 [scrapy] INFO: Closing spider (finished)\n",
      "2017-01-19 09:58:06 [scrapy] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 268,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 29834,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2017, 1, 18, 22, 58, 6, 280226),\n",
      " 'log_count/DEBUG': 2,\n",
      " 'log_count/INFO': 7,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2017, 1, 18, 22, 58, 4, 623035)}\n",
      "2017-01-19 09:58:06 [scrapy] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 268,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 29834,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2017, 1, 18, 22, 58, 6, 280226),\n",
      " 'log_count/DEBUG': 2,\n",
      " 'log_count/INFO': 7,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2017, 1, 18, 22, 58, 4, 623035)}\n",
      "2017-01-19 09:58:06 [scrapy] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 268,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 29834,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2017, 1, 18, 22, 58, 6, 280226),\n",
      " 'log_count/DEBUG': 2,\n",
      " 'log_count/INFO': 7,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2017, 1, 18, 22, 58, 4, 623035)}\n",
      "2017-01-19 09:58:06 [scrapy] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 268,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 29834,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2017, 1, 18, 22, 58, 6, 280226),\n",
      " 'log_count/DEBUG': 2,\n",
      " 'log_count/INFO': 7,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2017, 1, 18, 22, 58, 4, 623035)}\n",
      "2017-01-19 09:58:06 [scrapy] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 268,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 29834,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2017, 1, 18, 22, 58, 6, 280226),\n",
      " 'log_count/DEBUG': 2,\n",
      " 'log_count/INFO': 7,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2017, 1, 18, 22, 58, 4, 623035)}\n",
      "2017-01-19 09:58:06 [scrapy] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 268,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 29834,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2017, 1, 18, 22, 58, 6, 280226),\n",
      " 'log_count/DEBUG': 2,\n",
      " 'log_count/INFO': 7,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2017, 1, 18, 22, 58, 4, 623035)}\n",
      "2017-01-19 09:58:06 [scrapy] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 268,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 29834,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2017, 1, 18, 22, 58, 6, 280226),\n",
      " 'log_count/DEBUG': 2,\n",
      " 'log_count/INFO': 7,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2017, 1, 18, 22, 58, 4, 623035)}\n",
      "2017-01-19 09:58:06 [scrapy] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 268,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 29834,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2017, 1, 18, 22, 58, 6, 280226),\n",
      " 'log_count/DEBUG': 2,\n",
      " 'log_count/INFO': 7,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2017, 1, 18, 22, 58, 4, 623035)}\n",
      "2017-01-19 09:58:06 [scrapy] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 268,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 29834,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2017, 1, 18, 22, 58, 6, 280226),\n",
      " 'log_count/DEBUG': 2,\n",
      " 'log_count/INFO': 7,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2017, 1, 18, 22, 58, 4, 623035)}\n",
      "2017-01-19 09:58:06 [scrapy] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 268,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 29834,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2017, 1, 18, 22, 58, 6, 280226),\n",
      " 'log_count/DEBUG': 2,\n",
      " 'log_count/INFO': 7,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2017, 1, 18, 22, 58, 4, 623035)}\n",
      "2017-01-19 09:58:06 [scrapy] INFO: Spider closed (finished)\n",
      "2017-01-19 09:58:06 [scrapy] INFO: Spider closed (finished)\n",
      "2017-01-19 09:58:06 [scrapy] INFO: Spider closed (finished)\n",
      "2017-01-19 09:58:06 [scrapy] INFO: Spider closed (finished)\n",
      "2017-01-19 09:58:06 [scrapy] INFO: Spider closed (finished)\n",
      "2017-01-19 09:58:06 [scrapy] INFO: Spider closed (finished)\n",
      "2017-01-19 09:58:06 [scrapy] INFO: Spider closed (finished)\n",
      "2017-01-19 09:58:06 [scrapy] INFO: Spider closed (finished)\n",
      "2017-01-19 09:58:06 [scrapy] INFO: Spider closed (finished)\n",
      "2017-01-19 09:58:06 [scrapy] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    result_queue = Queue()\n",
    "    crawler = CrawlerWorker(CanberraBusinessSpider(), result_queue)\n",
    "    crawler.start()\n",
    "    for item in result_queue.get():\n",
    "        print item\n",
    "        \n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
