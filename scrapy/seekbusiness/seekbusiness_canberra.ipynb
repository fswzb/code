{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jun_gentoo/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:4: ScrapyDeprecationWarning: Importing from scrapy.xlib.pydispatch is deprecated and will no longer be supported in future Scrapy versions. If you just want to connect signals use the from_crawler class method, otherwise import pydispatch directly if needed. See: https://github.com/scrapy/scrapy/issues/1762\n"
     ]
    }
   ],
   "source": [
    "from scrapy import signals\n",
    "from scrapy.utils.project import get_project_settings\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.xlib.pydispatch import dispatcher\n",
    "from multiprocessing.queues import Queue\n",
    "import scrapy\n",
    "import multiprocessing\n",
    "import datetime\n",
    "import collections\n",
    "import urllib\n",
    "from scrapy import Request as Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CrawlerWorker(multiprocessing.Process):\n",
    "\n",
    "    def __init__(self, spider, result_queue):\n",
    "        multiprocessing.Process.__init__(self)\n",
    "        self.result_queue = result_queue\n",
    "\n",
    "        self.crawler = CrawlerProcess(get_project_settings())\n",
    "        #if not hasattr(project, 'crawler'):\n",
    "        #    self.crawler.install()\n",
    "        #self.crawler.configure()\n",
    "\n",
    "        self.items = []\n",
    "        self.spider = spider\n",
    "        dispatcher.connect(self._item_passed, signals.item_passed)\n",
    "\n",
    "    def _item_passed(self, item):\n",
    "        self.items.append(item)\n",
    "\n",
    "    def run(self):\n",
    "        self.crawler.crawl(self.spider)\n",
    "        self.crawler.start()\n",
    "        self.crawler.stop()\n",
    "        self.result_queue.put(self.items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CanberraBusinessSpider(scrapy.Spider):\n",
    "    name = \"CanberraBusinessSpider\"\n",
    "    allowed_domains = [\"www.seekbusiness.com.au\"]\n",
    "    start_urls = ['https://www.seekbusiness.com.au/businesses-for-sale/in-canberra-act-2601?rad=50']\n",
    "\n",
    "    def parse(self, response):\n",
    "        next_page=response.xpath('//a[contains(@class,\"next\")]/@href').extract()[0]\n",
    "        next_page_link=urllib.basejoin(response.url,next_page)\n",
    "        #print(next_page_link)\n",
    "        #yield Request(next_page_link,callback=self.parse)\n",
    "        business_tags=response.xpath('//a[contains(@href,\"business-listing\") and count(@*)=1]')\n",
    "        for invidual_business in business_tags:\n",
    "            title=invidual_business.xpath('text()').extract()[0]\n",
    "            link=invidual_business.xpath('@href').extract()[0]\n",
    "            full_link=urllib.basejoin(response.url,link)\n",
    "            business={'title':title,'link':full_link}\n",
    "            print(business)\n",
    "        #print(business_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-17 16:37:47 [scrapy] INFO: Scrapy 1.1.1 started (bot: scrapybot)\n",
      "2017-01-17 16:37:47 [scrapy] INFO: Scrapy 1.1.1 started (bot: scrapybot)\n",
      "2017-01-17 16:37:47 [scrapy] INFO: Scrapy 1.1.1 started (bot: scrapybot)\n",
      "2017-01-17 16:37:47 [scrapy] INFO: Scrapy 1.1.1 started (bot: scrapybot)\n",
      "2017-01-17 16:37:47 [scrapy] INFO: Scrapy 1.1.1 started (bot: scrapybot)\n",
      "2017-01-17 16:37:47 [scrapy] INFO: Overridden settings: {}\n",
      "2017-01-17 16:37:47 [scrapy] INFO: Overridden settings: {}\n",
      "2017-01-17 16:37:47 [scrapy] INFO: Overridden settings: {}\n",
      "2017-01-17 16:37:47 [scrapy] INFO: Overridden settings: {}\n",
      "2017-01-17 16:37:47 [scrapy] INFO: Overridden settings: {}\n",
      "2017-01-17 16:37:47 [scrapy] INFO: Enabled extensions:\n",
      "['scrapy.extensions.logstats.LogStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.corestats.CoreStats']\n",
      "2017-01-17 16:37:47 [scrapy] INFO: Enabled extensions:\n",
      "['scrapy.extensions.logstats.LogStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.corestats.CoreStats']\n",
      "2017-01-17 16:37:47 [scrapy] INFO: Enabled extensions:\n",
      "['scrapy.extensions.logstats.LogStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.corestats.CoreStats']\n",
      "2017-01-17 16:37:47 [scrapy] INFO: Enabled extensions:\n",
      "['scrapy.extensions.logstats.LogStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.corestats.CoreStats']\n",
      "2017-01-17 16:37:47 [scrapy] INFO: Enabled extensions:\n",
      "['scrapy.extensions.logstats.LogStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.corestats.CoreStats']\n",
      "2017-01-17 16:37:47 [scrapy] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2017-01-17 16:37:47 [scrapy] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2017-01-17 16:37:47 [scrapy] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2017-01-17 16:37:47 [scrapy] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2017-01-17 16:37:47 [scrapy] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2017-01-17 16:37:47 [scrapy] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2017-01-17 16:37:47 [scrapy] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2017-01-17 16:37:47 [scrapy] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2017-01-17 16:37:47 [scrapy] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2017-01-17 16:37:47 [scrapy] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2017-01-17 16:37:47 [scrapy] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2017-01-17 16:37:47 [scrapy] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2017-01-17 16:37:47 [scrapy] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2017-01-17 16:37:47 [scrapy] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2017-01-17 16:37:47 [scrapy] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2017-01-17 16:37:47 [scrapy] INFO: Spider opened\n",
      "2017-01-17 16:37:47 [scrapy] INFO: Spider opened\n",
      "2017-01-17 16:37:47 [scrapy] INFO: Spider opened\n",
      "2017-01-17 16:37:47 [scrapy] INFO: Spider opened\n",
      "2017-01-17 16:37:47 [scrapy] INFO: Spider opened\n",
      "2017-01-17 16:37:47 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2017-01-17 16:37:47 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2017-01-17 16:37:47 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2017-01-17 16:37:47 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2017-01-17 16:37:47 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2017-01-17 16:37:47 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023\n",
      "2017-01-17 16:37:47 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023\n",
      "2017-01-17 16:37:47 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023\n",
      "2017-01-17 16:37:47 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023\n",
      "2017-01-17 16:37:47 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023\n",
      "2017-01-17 16:37:49 [scrapy] DEBUG: Crawled (200) <GET https://www.seekbusiness.com.au/businesses-for-sale/in-canberra-act-2601?rad=50> (referer: None)\n",
      "2017-01-17 16:37:49 [scrapy] DEBUG: Crawled (200) <GET https://www.seekbusiness.com.au/businesses-for-sale/in-canberra-act-2601?rad=50> (referer: None)\n",
      "2017-01-17 16:37:49 [scrapy] DEBUG: Crawled (200) <GET https://www.seekbusiness.com.au/businesses-for-sale/in-canberra-act-2601?rad=50> (referer: None)\n",
      "2017-01-17 16:37:49 [scrapy] DEBUG: Crawled (200) <GET https://www.seekbusiness.com.au/businesses-for-sale/in-canberra-act-2601?rad=50> (referer: None)\n",
      "2017-01-17 16:37:49 [scrapy] DEBUG: Crawled (200) <GET https://www.seekbusiness.com.au/businesses-for-sale/in-canberra-act-2601?rad=50> (referer: None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'link': u'https://www.seekbusiness.com.au/business-listing/mos-mobiles-is-looking-for-licensed-dealers-to-operate-existing-vodafone-stores/281849?s=3001586', 'title': u\"Mo's Mobiles is looking for Licensed Dealers to operate existing Vodafone stores\"}\n",
      "{'link': u'https://www.seekbusiness.com.au/business-listing/gozleme-king-franchise-turkish-street-food-cafe/270622?s=3001586', 'title': u'Gozleme King Franchise : Turkish Street Food & Cafe'}\n",
      "{'link': u'https://www.seekbusiness.com.au/business-listing/healthy-start-civic/258794?s=3001586', 'title': u'Healthy Start Civic'}\n",
      "{'link': u'https://www.seekbusiness.com.au/business-listing/the-fat-goanna-cafe/290623?s=3001586', 'title': u'The Fat Goanna Cafe'}\n",
      "{'link': u'https://www.seekbusiness.com.au/business-listing/community-sector-rto-for-sale-delegate-authority-funding-2-states-excellent/286398?s=3001586', 'title': u'Community sector RTO for sale. Delegate Authority. Funding- 2 states. Excellent!'}\n",
      "{'link': u'https://www.seekbusiness.com.au/business-listing/rent-roll-sale-80-100-managements/264886?s=3001586', 'title': u'Rent Roll Sale 80-100 Managements'}\n",
      "{'link': u'https://www.seekbusiness.com.au/business-listing/lollipops-playland-childrens-indoor-play-centre-and-cafe-franchise/107888?s=3001586', 'title': u\"Lollipop's Playland - Children's Indoor Play centre and Caf\\xe9 franchise\"}\n",
      "{'link': u'https://www.seekbusiness.com.au/business-listing/sumo-salad-tuggeranong-a-successful-absentee-owner-food-franchise/286193?s=3001586', 'title': u'Sumo Salad Tuggeranong: a successful absentee owner food franchise'}\n",
      "{'link': u'https://www.seekbusiness.com.au/business-listing/exciting-opportunity-to-own-and-manage-your-own-skin-laser-clinic/289893?s=3001586', 'title': u'Exciting opportunity to own and manage your own Skin & Laser clinic!!!'}\n",
      "{'link': u'https://www.seekbusiness.com.au/business-listing/own-your-own-rto-today/239855?s=3001586', 'title': u'Own your own RTO today!'}\n",
      "{'link': u'https://www.seekbusiness.com.au/business-listing/southside-supermarket/205055?s=3001586', 'title': u'Southside Supermarket'}\n",
      "{'link': u'https://www.seekbusiness.com.au/business-listing/ace-sushi-braddon/289743?s=3001586', 'title': u'Ace Sushi Braddon'}\n",
      "{'link': u'https://www.seekbusiness.com.au/business-listing/supercheap-storage-l-mobile-self-storage-l-simple-management-highly-profitable/222538?s=3001586', 'title': u'Supercheap Storage l Mobile self-storage l Simple management, highly profitable!'}\n",
      "{'link': u'https://www.seekbusiness.com.au/business-listing/japanese-tapas-bar/289567?s=3001586', 'title': u'Japanese Tapas Bar'}\n",
      "{'link': u'https://www.seekbusiness.com.au/business-listing/sumo-salad-woden/289538?s=3001586', 'title': u'Sumo Salad Woden'}\n",
      "{'link': u'https://www.seekbusiness.com.au/business-listing/cafecoffeeshop-canberra/269411?s=3001586', 'title': u'CafeCoffeeShop - Canberra'}\n",
      "{'link': u'https://www.seekbusiness.com.au/business-listing/furniture-direct-warehouse/289390?s=3001586', 'title': u'Furniture Direct Warehouse'}\n",
      "{'link': u'https://www.seekbusiness.com.au/business-listing/ogilvie-automotive-and-exhaust/289371?s=3001586', 'title': u'Ogilvie Automotive and Exhaust'}\n",
      "{'link': u'https://www.seekbusiness.com.au/business-listing/jet-flight-simulator-canberra/289368?s=3001586', 'title': u'Jet Flight Simulator Canberra'}\n",
      "{'link': u'https://www.seekbusiness.com.au/business-listing/coffee-guru-canberra-centre/289332?s=3001586', 'title': u'Coffee Guru Canberra Centre'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-17 16:37:49 [scrapy] INFO: Closing spider (finished)\n",
      "2017-01-17 16:37:49 [scrapy] INFO: Closing spider (finished)\n",
      "2017-01-17 16:37:49 [scrapy] INFO: Closing spider (finished)\n",
      "2017-01-17 16:37:49 [scrapy] INFO: Closing spider (finished)\n",
      "2017-01-17 16:37:49 [scrapy] INFO: Closing spider (finished)\n",
      "2017-01-17 16:37:50 [scrapy] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 268,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 28938,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2017, 1, 17, 5, 37, 50, 29701),\n",
      " 'log_count/DEBUG': 2,\n",
      " 'log_count/INFO': 7,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2017, 1, 17, 5, 37, 47, 833044)}\n",
      "2017-01-17 16:37:50 [scrapy] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 268,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 28938,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2017, 1, 17, 5, 37, 50, 29701),\n",
      " 'log_count/DEBUG': 2,\n",
      " 'log_count/INFO': 7,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2017, 1, 17, 5, 37, 47, 833044)}\n",
      "2017-01-17 16:37:50 [scrapy] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 268,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 28938,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2017, 1, 17, 5, 37, 50, 29701),\n",
      " 'log_count/DEBUG': 2,\n",
      " 'log_count/INFO': 7,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2017, 1, 17, 5, 37, 47, 833044)}\n",
      "2017-01-17 16:37:50 [scrapy] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 268,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 28938,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2017, 1, 17, 5, 37, 50, 29701),\n",
      " 'log_count/DEBUG': 2,\n",
      " 'log_count/INFO': 7,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2017, 1, 17, 5, 37, 47, 833044)}\n",
      "2017-01-17 16:37:50 [scrapy] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 268,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 28938,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2017, 1, 17, 5, 37, 50, 29701),\n",
      " 'log_count/DEBUG': 2,\n",
      " 'log_count/INFO': 7,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2017, 1, 17, 5, 37, 47, 833044)}\n",
      "2017-01-17 16:37:50 [scrapy] INFO: Spider closed (finished)\n",
      "2017-01-17 16:37:50 [scrapy] INFO: Spider closed (finished)\n",
      "2017-01-17 16:37:50 [scrapy] INFO: Spider closed (finished)\n",
      "2017-01-17 16:37:50 [scrapy] INFO: Spider closed (finished)\n",
      "2017-01-17 16:37:50 [scrapy] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    result_queue = Queue()\n",
    "    crawler = CrawlerWorker(CanberraBusinessSpider(), result_queue)\n",
    "    crawler.start()\n",
    "    for item in result_queue.get():\n",
    "        print item\n",
    "        \n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
